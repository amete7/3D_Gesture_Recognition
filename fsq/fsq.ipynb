{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Xy0vuPg-gS"
      },
      "source": [
        "```\n",
        "Copyright 2023 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_XzbBLngvoM"
      },
      "source": [
        "# FSQ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmQq4IJKBSiO"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(\n",
        "        ([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "\n",
        "    self._implicit_codebook = self.indexes_to_codes(\n",
        "        np.arange(self.codebook_size))\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self) -> int:\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self) -> int:\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    codes_non_centered = np.mod(\n",
        "        np.floor_divide(indices, self._basis), self._levels_np\n",
        "    )\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0JkjUv6Rl0Z"
      },
      "source": [
        "# Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkrLkcvbRnbN"
      },
      "outputs": [],
      "source": [
        "fsq = FSQ(levels=[3, 5, 4])\n",
        "\n",
        "z = np.asarray([0.25, 0.6, -7])\n",
        "print(z.shape)\n",
        "zhat = fsq.quantize(z)\n",
        "print(f\"Quantized {z} -> {zhat}\")\n",
        "\n",
        "# We can map to an index in the codebook.\n",
        "idx = fsq.codes_to_indexes(zhat)\n",
        "print(f\"Code {zhat} is the {idx}-th index.\")\n",
        "\n",
        "# Back to code\n",
        "code_out = fsq.indexes_to_codes(idx)\n",
        "print(f\"Index {idx} mapped back to {zhat}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnQBFfftSmyy"
      },
      "source": [
        "# Quantizing a multi-dimensional bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkJxytDvSqqL"
      },
      "outputs": [],
      "source": [
        "fsq = FSQ(levels=[5, 4, 3])\n",
        "\n",
        "d = fsq.num_dimensions\n",
        "print(d)\n",
        "z = np.random.uniform(size=(3, 8, 8, d))\n",
        "zhat = fsq.quantize(z)\n",
        "assert zhat.shape == (3, 8, 8, d)\n",
        "\n",
        "indices = fsq.codes_to_indexes(zhat)\n",
        "assert indices.shape == (3, 8, 8)\n",
        "\n",
        "zhat_out = fsq.indexes_to_codes(indices)\n",
        "assert zhat_out.shape == zhat.shape\n",
        "\n",
        "np.testing.assert_allclose(zhat, zhat_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5BuPjRsTbjK"
      },
      "source": [
        "# Validating codebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epeDl8MmRqSj"
      },
      "outputs": [],
      "source": [
        "fsq = FSQ(levels=[3, 4])\n",
        "print(fsq.codebook)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vector_quantize_pytorch import VectorQuantize\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ],
      "metadata": {
        "id": "NgWrl3aTHWK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
      ],
      "metadata": {
        "id": "3cBNKKdvTUBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs = torch.randn(2,10,3,244,244)\n",
        "obs = obs.view(20,3,244,244)\n",
        "obs_embed = model.get_image_features(obs)\n",
        "obs_embed = obs_embed.view(2, 10, -1)"
      ],
      "metadata": {
        "id": "WPhh2vQNTT5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_embed.shape"
      ],
      "metadata": {
        "id": "as8JBhAKVBL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Assume you have image embeddings with shape [2, 10, 512]\n",
        "# image_embeddings = torch.rand((2, 10, 512))\n",
        "\n",
        "# Define the Transformer Encoder model\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_size, num_layers):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_size, nhead=2, batch_first=True),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the Transformer Encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "# Define the Transformer Encoder model with the specified input size and number of layers\n",
        "transformer_model = TransformerEncoderModel(input_size=512, num_layers=2)\n",
        "\n",
        "# Forward pass through the Transformer Encoder with the image embeddings\n",
        "encoder_output = transformer_model(obs_embed)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Transformer Encoder output shape:\", encoder_output.shape)"
      ],
      "metadata": {
        "id": "VrVI9I53bRB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = encoder_output.view(2, 10*512)"
      ],
      "metadata": {
        "id": "u51hgngPoo-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "ggzUUb45oxaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_head = nn.Linear(10*512, 4)\n",
        "out = prediction_head(x)"
      ],
      "metadata": {
        "id": "zBocRuRLo8w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "id": "LferR1zypJBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = out.unsqueeze(0)"
      ],
      "metadata": {
        "id": "jLFBO8cqrcPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zt = torch.randn(1,2,4)\n",
        "quantizer(zt)"
      ],
      "metadata": {
        "id": "8aetpAYFq1OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "levels = [8,5,5,5] # see 4.1 and A.4.1 in the paper\n",
        "quantizer = FSQ(levels)\n",
        "xhat, indices = quantizer(out)"
      ],
      "metadata": {
        "id": "cBqxmByapL-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Assume you have image embeddings with shape [2, 10, 512]\n",
        "# image_embeddings = torch.rand((2, 10, 512))\n",
        "\n",
        "# Define the Transformer Encoder model\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_size, nhead=2, batch_first=True),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the Transformer Encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Apply linear layers\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define the Transformer Encoder model with the specified input size, hidden size, output size, and number of layers\n",
        "transformer_model = TransformerEncoderModel(input_size=512, hidden_size=256, output_size=4, num_layers=2)\n",
        "\n",
        "# Forward pass through the Transformer Encoder with the image embeddings\n",
        "encoder_output = transformer_model(obs_embed)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Transformer Encoder output shape:\", encoder_output.shape)"
      ],
      "metadata": {
        "id": "yiCyImXCoctk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "levels = [8,5,5,5] # see 4.1 and A.4.1 in the paper\n",
        "quantizer = FSQ(levels)\n",
        "\n",
        "# x = torch.randn(1, 1024, 4) # 4 since there are 4 levels\n",
        "xhat, indices = quantizer(encoder_output)"
      ],
      "metadata": {
        "id": "6rYV0VBHi_Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xhat.shape)    # (1, 1024, 4) - (batch, seq, dim)\n",
        "print(indices.shape)"
      ],
      "metadata": {
        "id": "zRmmCcQVjS4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xhat"
      ],
      "metadata": {
        "id": "YGt0Sp09nzAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "id": "TSFRUyqzjWlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSpcUjTeHpgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70g5MmXWiYMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vq = VectorQuantize(\n",
        "    dim = 512,\n",
        "    codebook_size = 512,     # codebook size\n",
        "    decay = 0.8,             # the exponential moving average decay, lower means the dictionary will change faster\n",
        "    commitment_weight = 1.   # the weight on the commitment loss\n",
        ")"
      ],
      "metadata": {
        "id": "TcRtfUhgbQ_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized, indices, commit_loss = vq(encoder_output)"
      ],
      "metadata": {
        "id": "LMFHArLAbQ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "id": "p_I8dkNRJRhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized.shape"
      ],
      "metadata": {
        "id": "YT0fSeCnHpnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgYKC48SHpjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5kXO00EiPu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8SQd0QV7Hpc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assume you have a tensor of shape [2, 10, 512]\n",
        "input_tensor = torch.rand((2, 10, 512))\n",
        "\n",
        "# Feature-wise Linear Modulation (FiLM)\n",
        "film_layer = nn.Linear(512, 4)\n",
        "output_tensor = film_layer(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape (FiLM):\", output_tensor.shape)\n"
      ],
      "metadata": {
        "id": "Mp_Q2X4pHpaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mipZmHXpmjjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7l84ZhOmjgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ESYF2tnmjdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtHSvs03HpW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vector-quantize-pytorch"
      ],
      "metadata": {
        "id": "-aLDjkWOHZDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcoQvyuQHkIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}